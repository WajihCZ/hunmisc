wikitext_to_conll.py:
--------------------

Parses Wikipedia dumps in the MediaWiki format. Creates two files, one is the
article in ConLL format; the other is the list of templates found in the files.
This latter file is pretty useless right now.

The most important parameter is -l/--language, the language of the markup.
Always set it to match the language of the Wikipedia dump, lest the result
become corrupt, or (much more likely) the script fails. Be sure to download
the required siteinfo.json file in mwlib with the script
mwlib*egg/mwlib/siteinfo/fetch_siteinfo.py.

The tools to use for each language can be specified in a configuration file.
A general section, named 'tools' is used to specify the default tools.

See wiki.conf for an example. In this setup, English text is tokenized, POS
tagged and lemmatized via NltkTools; Hungarian text is tokenized into sentences
with a ME-based chunker, and POS tagged and lemmatized by ocamorph and
hundisambig.

For an example for all options, please refer to the tool documentations in
../utils/tools.py and ../utils/language_config.py.

TODO:
= 1999. -> NUM, not NOUN

- different cleansing steps by language
- test with hunpos?
- unify it with to_ndavid.py
- refactor, it's a piece of shit
